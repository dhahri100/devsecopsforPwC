# Projet DevSecOps & IA ‚Äì PwC

Ce projet int√®gre **LiteLLM**, une application cliente, un **dashboard de s√©curit√©**, et **n8n** afin de d√©montrer un flux complet **DevSecOps & Intelligence Artificielle** pour PwC.  
Il est con√ßu pour √™tre lanc√© soit via **Docker**, soit en **local** sur la machine.

## Table des mati√®res
- [Installation et d√©marrage du projet](#installation-et-d√©marrage-du-projet)
  - [1. Lancer LiteLLM](#1-lancer-litellm)
    - [Option 1 : Lancer avec Docker](#option-1--lancer-avec-docker)
    - [Option 2 : Lancer en local (sans Docker)](#option-2--lancer-en-local-sans-docker)
  - [2. Lancer l‚Äôapplication client](#2-lancer-lapplication-client)
  - [3. Lancer le dashboard s√©curit√©](#3-lancer-le-dashboard-s√©curit√©)
  - [4. Lancer n8n](#4-lancer-n8n)

## Installation et d√©marrage du projet

### 1. Lancer LiteLLM
LiteLLM peut √™tre lanc√© de deux mani√®res :  
- **Option 1 : via Docker (recommand√©)**  
- **Option 2 : en local sur la machine PwC**

#### Option 1 : Lancer avec Docker
> Cette option est la plus simple et rapide pour d√©ployer LiteLLM.

1. Assurez-vous d‚Äô√™tre dans le r√©pertoire du projet o√π se trouve le fichier `docker-compose.yml`.
2. Lancez la commande suivante :
   ```bash
   docker-compose up -d --build
   ```
LiteLLM sera disponible sur :  
üëâ http://localhost:4000

#### Option 2 : Lancer en local (sans Docker)
Cette m√©thode est utile si vous souhaitez d√©marrer LiteLLM directement sur votre PC PwC sans conteneurisation.

1. Installer LiteLLM via pip :
   ```bash
   pip install litellm
   ```

2. D√©marrer le serveur LiteLLM avec un mod√®le  
   Exemple avec le mod√®le ollama/llama2 :
   ```bash
   litellm --model ollama/llama2
   ```

3. V√©rifier l‚Äôacc√®s √† l‚ÄôAPI LiteLLM  
   Acc√©dez √† : http://localhost:4000

‚ö†Ô∏è **Remarque importante** :  
Pour utiliser un mod√®le Ollama, vous devez installer Ollama au pr√©alable.

### 2. Lancer l‚Äôapplication client
Cette application permet d‚Äôinteragir avec LiteLLM via une interface web.

1. Acc√©dez au dossier du projet :
   ```bash
   cd prompt_Interface/
   cd frontend/
   ```

2. Lancez l‚Äôapplication :
   ```bash
   npm start
   ```

L‚Äôapplication sera disponible sur http://localhost:3000.  
Cette √©tape fonctionne avec LiteLLM lanc√© via Docker ou en local.

### 3. Lancer le dashboard s√©curit√©
Le dashboard permet de visualiser les flux et la s√©curit√© des appels IA.

1. Acc√©dez au r√©pertoire d√©di√© :
   ```bash
   cd prompt_Interface/
   cd frontend/
   cd ai-flow-guard/
   ```

2. D√©marrez le dashboard :
   ```bash
   npm run dev
   ```

Dashboard disponible sur http://localhost:5173.

### 4. Lancer n8n
n8n est utilis√© pour l‚Äôorchestration et l‚Äôautomatisation des workflows.

1. Lancez n8n via Git Bash :
   ```bash
   n8n
   ```

2. Acc√©dez √† l‚Äôinterface web de n8n :  
   üëâ http://localhost:5678

3. Importez votre workflow dans n8n.

4. Ex√©cutez le workflow pour orchestrer vos t√¢ches.

